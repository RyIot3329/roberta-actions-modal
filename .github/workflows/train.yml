# .github/workflows/train.yml
#
# Fine-tune RoBERTa on Modal with data preprocessing
# Creates a pull request with training results
# Pushes model to Hugging Face Hub

name: Fine-tune RoBERTa

on:
  workflow_dispatch:
    inputs:
      epochs:
        description: "Number of training epochs"
        required: false
        default: "9"
      batch_size:
        description: "Batch size"
        required: false
        default: "2"
      learning_rate:
        description: "Learning rate"
        required: false
        default: "1e-5"
      max_seq_length:
        description: "Maximum sequence length"
        required: false
        default: "25"
      optimizer:
        description: "Optimizer"
        required: false
        default: "adamw_torch"
        type: choice
        options:
          - "adamw_torch"
          - "adamw_hf"
          - "sgd"
          - "adafactor"
      scheduler:
        description: "Learning rate scheduler"
        required: false
        default: "linear"
        type: choice
        options:
          - "linear"
          - "cosine"
          - "cosine_with_restarts"
          - "polynomial"
          - "constant"
          - "constant_with_warmup"
      gradient_accumulation:
        description: "Gradient accumulation steps"
        required: false
        default: "8"
      mixed_precision:
        description: "Mixed precision training"
        required: false
        default: "fp16"
        type: choice
        options:
          - "fp16"
          - "bf16"
          - "no"
      weight_decay:
        description: "Weight decay"
        required: false
        default: "0.075"
      push_to_hub:
        description: "Push model to Hugging Face Hub"
        required: false
        default: "true"
        type: choice
        options:
          - "true"
          - "false"
      hf_repo:
        description: "Hugging Face repo (username/model-name)"
        required: false
        default: "RyIoT33/haystack-autotagging"

  push:
    branches: [main]
    paths:
      - "data/**"
      - "scripts/**"

env:
  PYTHON_VERSION: "3.10"
  HF_REPO: ${{ github.event.inputs.hf_repo || 'RyIoT33/haystack-autotagging' }}
  PUSH_TO_HUB: ${{ github.event.inputs.push_to_hub || 'true' }}
  # Training defaults
  EPOCHS: ${{ github.event.inputs.epochs || '9' }}
  BATCH_SIZE: ${{ github.event.inputs.batch_size || '2' }}
  LEARNING_RATE: ${{ github.event.inputs.learning_rate || '1e-5' }}
  MAX_SEQ_LENGTH: ${{ github.event.inputs.max_seq_length || '25' }}
  OPTIMIZER: ${{ github.event.inputs.optimizer || 'adamw_torch' }}
  SCHEDULER: ${{ github.event.inputs.scheduler || 'linear' }}
  GRADIENT_ACCUMULATION: ${{ github.event.inputs.gradient_accumulation || '8' }}
  MIXED_PRECISION: ${{ github.event.inputs.mixed_precision || 'fp16' }}
  WEIGHT_DECAY: ${{ github.event.inputs.weight_decay || '0.075' }}

jobs:
  train:
    name: Preprocess & Fine-tune on Modal
    runs-on: ubuntu-latest

    permissions:
      contents: write
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install modal

      - name: Configure Modal
        run: |
          modal token set \
            --token-id ${{ secrets.MODAL_TOKEN_ID }} \
            --token-secret ${{ secrets.MODAL_TOKEN_SECRET }}

      # ============================================
      # Data Preprocessing Steps
      # ============================================

      - name: "Step 1: Clean Data"
        run: |
          echo "Cleaning data..."
          python scripts/clean_data.py

      - name: "Step 2: Print Tags"
        run: |
          echo "Printing label distribution..."
          python scripts/print_tags.py

      - name: "Step 3: Train/Valid Split"
        run: |
          echo "Splitting data..."
          python scripts/train_split.py

      - name: "Step 4: Convert to JSONL"
        run: |
          echo "Converting to JSONL format..."
          python scripts/convert_to_jsonl.py

      # ============================================
      # Modal Training
      # ============================================

      - name: Create output directory
        run: mkdir -p output

      - name: Run fine-tuning on Modal
        id: train
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "Starting Modal training..."
          echo "========================================"
          echo "Training Configuration:"
          echo "  Epochs: ${{ env.EPOCHS }}"
          echo "  Batch size: ${{ env.BATCH_SIZE }}"
          echo "  Learning rate: ${{ env.LEARNING_RATE }}"
          echo "  Max seq length: ${{ env.MAX_SEQ_LENGTH }}"
          echo "  Optimizer: ${{ env.OPTIMIZER }}"
          echo "  Scheduler: ${{ env.SCHEDULER }}"
          echo "  Gradient accumulation: ${{ env.GRADIENT_ACCUMULATION }}"
          echo "  Mixed precision: ${{ env.MIXED_PRECISION }}"
          echo "  Weight decay: ${{ env.WEIGHT_DECAY }}"
          echo "========================================"

          # Build the command
          CMD="modal run ./scripts/finetune.py"
          CMD="$CMD --epochs ${{ env.EPOCHS }}"
          CMD="$CMD --batch-size ${{ env.BATCH_SIZE }}"
          CMD="$CMD --learning-rate ${{ env.LEARNING_RATE }}"
          CMD="$CMD --max-seq-length ${{ env.MAX_SEQ_LENGTH }}"
          CMD="$CMD --optimizer ${{ env.OPTIMIZER }}"
          CMD="$CMD --scheduler ${{ env.SCHEDULER }}"
          CMD="$CMD --gradient-accumulation ${{ env.GRADIENT_ACCUMULATION }}"
          CMD="$CMD --mixed-precision ${{ env.MIXED_PRECISION }}"
          CMD="$CMD --weight-decay ${{ env.WEIGHT_DECAY }}"

          # Add HF Hub flags if pushing
          if [ "${{ env.PUSH_TO_HUB }}" == "true" ]; then
            CMD="$CMD --push-to-hub --hf-repo ${{ env.HF_REPO }}"
          fi

          echo "Running: $CMD"
          eval $CMD

          # Get the output filename
          OUTPUT_FILE=$(ls -t output/*.txt | head -1)
          echo "output_file=$OUTPUT_FILE" >> $GITHUB_OUTPUT
          echo "output_filename=$(basename $OUTPUT_FILE)" >> $GITHUB_OUTPUT

          # Add to workflow summary
          echo "## Training Results" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat "$OUTPUT_FILE" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

          if [ "${{ env.PUSH_TO_HUB }}" == "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Hugging Face Model" >> $GITHUB_STEP_SUMMARY
            echo "ðŸ¤— https://huggingface.co/${{ env.HF_REPO }}" >> $GITHUB_STEP_SUMMARY
          fi

      # ============================================
      # Commit Results & Create PR
      # ============================================

      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          add-paths: |
            data/
            output/
          commit-message: |
            Add training results and processed data

            - Preprocessed data files
            - Training results: ${{ steps.train.outputs.output_filename }}
            - Epochs: ${{ env.EPOCHS }}
            - Batch size: ${{ env.BATCH_SIZE }}
            - Learning rate: ${{ env.LEARNING_RATE }}
            - Model pushed to: ${{ env.HF_REPO }}
          branch: training-results-${{ github.run_id }}
          delete-branch: true
          title: "ðŸ¤– Training Results - ${{ steps.train.outputs.output_filename }}"
          body: |
            ## Automated Training Results

            Training completed successfully on Modal.

            ### Configuration
            | Parameter | Value |
            |-----------|-------|
            | Epochs | ${{ env.EPOCHS }} |
            | Batch size | ${{ env.BATCH_SIZE }} |
            | Learning rate | ${{ env.LEARNING_RATE }} |
            | Max seq length | ${{ env.MAX_SEQ_LENGTH }} |
            | Optimizer | ${{ env.OPTIMIZER }} |
            | Scheduler | ${{ env.SCHEDULER }} |
            | Gradient accumulation | ${{ env.GRADIENT_ACCUMULATION }} |
            | Mixed precision | ${{ env.MIXED_PRECISION }} |
            | Weight decay | ${{ env.WEIGHT_DECAY }} |

            ### Model
            ðŸ¤— **Hugging Face:** https://huggingface.co/${{ env.HF_REPO }}

            ### Files Updated
            - `data/cleaned_data.csv` - Cleaned training data
            - `data/train.csv` / `data/valid.csv` - Train/validation CSV splits
            - `data/train.jsonl` / `data/validation.jsonl` / `data/test.jsonl` - JSONL format
            - `data/label_mapping.json` - Label ID mappings
            - `data/dataset_summary.json` - Dataset statistics
            - `${{ steps.train.outputs.output_file }}` - Training results

            ### Results
            ðŸ“Š [View full results in workflow summary](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          labels: |
            automated
            training-results

      - name: Summary
        run: |
          echo "=========================================="
          echo "PIPELINE COMPLETE"
          echo "=========================================="
          echo "Output: ${{ steps.train.outputs.output_file }}"
          echo "Model: https://huggingface.co/${{ env.HF_REPO }}"
          echo "PR created for branch: training-results-${{ github.run_id }}"
          echo "=========================================="
